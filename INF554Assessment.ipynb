{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15add333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import itertools\n",
    "from random import sample\n",
    "import math\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1939df26-284c-4b1f-8dfe-0f97f5382248",
   "metadata": {},
   "source": [
    "# INF554 Assessment: Trees, Bagging and Boosting\n",
    "\n",
    "In this assessment it is your task to delve deeper into the construction of regression tree-based models.\n",
    "\n",
    "**Please submit a zip file containing the following to moodle by the 4th November 2024 14.00 (Paris time):**\n",
    "\n",
    "- your filled out assessment jupyter notebook on the proposed coding tasks and short written answers to the questions;\n",
    "- all the files necessary to run the jupyter notebook properly.\n",
    "\n",
    "Make sure to submit a couple of hours ahead of the deadline to ensure that technical difficulties do not cause you to miss the deadline. Late submissions will not be accepted.\n",
    "\n",
    "Please note that this assessment is *to be completed individually. We will forward detected cases of plagarism to the university,* which in serious cases can have farreaching consequences for you. So, please make sure to submit your own, original solutions to this assessment. \n",
    "\n",
    "*Disclaimer:* You will not receive marks for importing the functions that you are asked to code from any library, unless specifically stated in the corresponding task. In this assessment, we ask you to code these methods from scratch in the hope that this will give you a better understanding of them.\n",
    "\n",
    "## Data set\n",
    "\n",
    "In this assessment, you will be working with the <a href=\"https://www.kaggle.com/code/prasadperera/the-boston-housing-dataset\">Boston-Housing</a> dataset which originates from a study that aimed to predict the median value of owner-occupied homes in thousands of dollars (medv) in 506 observations from census tracts of the Boston metropolitan area. For this purpose, the followig measures were considered: crime rate by town (crim, $X_1$), proportion of residential land zoned for lots greater than 25,000 sq ft (zne, $X_2$), proportion of nonretail business acres per town (indus, $X_3$), whether the tract bounds river (1) or not (0) (chass, $X_4$), nitrogen oxide concentration in parts per hundred million (nox, $X_5$), average number of rooms in owner units (rm, $X_6$), proportion of owner units built prior to $1940$ (age, $X_7$), weighted distances to five employment centers in the Boston region (dis, $X_8$), index of accessibility to radial highways (rad, $X_9$), full value property tax rate per ten thousands of dollars (tax, $X_{10}$), pupil-teacher ratio by town school district (ptratio, $X_{11}$), black proportion of population (b, $X_{12}$) and proportion of population that is lower status (lstat, $X_{13}$). Thus, we have $13$ predictor variables and the continuous response variable medv, which we denote by $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d2c490f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      crim    zn  indus  chas    nox     rm   age     dis  rad  tax  ptratio  \\\n",
      "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
      "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
      "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
      "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
      "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
      "\n",
      "        b  lstat  medv  \n",
      "0  396.90   4.98  24.0  \n",
      "1  396.90   9.14  21.6  \n",
      "2  392.83   4.03  34.7  \n",
      "3  394.63   2.94  33.4  \n",
      "4  396.90   5.33  36.2  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"BostonHousing.csv\")\n",
    "print(df.head())\n",
    "features = df.columns[0:(len(df.columns)-1)]\n",
    "X = np.array(df[features])\n",
    "y = np.array(df.medv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff3c80a",
   "metadata": {},
   "source": [
    "## A. Trees\n",
    "\n",
    "In this section, we will learn how to construct a regression tree. Suppose we are given a training sample of $N$ individuals, $\\left\\lbrace \\left(x_i,y_i \\right) \\right\\rbrace_{1\\leq i\\leq N}$, on which $d$ predictor variables are measured, $x_i\\in\\mathbb{R}^d$, and one real value is associated with each one, $y_i\\in\\mathbb{R}$. Classic regression tree methods consist of sequentially, and greedily, partitioning the predictor space $\\mathbb{R}^d$ into disjoint sets or *nodes* by imposing certain conditions on the predictor variables. The usual splitting criterion is to take the split that produces descendant nodes with smaller and smaller prediction errors. The process of partitioning finishes when a stopping criterion is satisfied. Then, leaf nodes are labeled with a numerical value. Commonly, a leaf node is labeled with a constant, such as the mean or the median variable response in the individuals that have fallen into the node. Once the tree is built, the prediction of future unlabeled data is done in a deterministic way. Given a new observation, starting from the root node, it will end up in a leaf node, depending on the values the predictor variables take, and its predicted value will be the one attached to that leaf node. Because of the construction of regression trees, normalization of the data is not required.\n",
    "\n",
    "Specifically, we will focus on binary regression trees with univariate splits. Given a node $t$, the proposed splitting for continuous features is to generate the following left and right child nodes: $t_L = \\left\\lbrace u\\in t: x_{ju} \\leq b_t\\right\\rbrace$ and $t_R = \\left\\lbrace u\\in t: x_{ju} > b_t\\right\\rbrace$, where cutpoint $b_t$ is the halfway point between two consecutive data values of feature $x_j$. Assuming that these values are ordered from lowest to highest, at most $N-1$ different splits can be done. As said, the best split will be the one that best-separates the objects in the training sample by their response variable values, that is, that produces the maximum error reduction of objects associated with resultant nodes. In the following, we study different quantitative ways to measure the error within a node.\n",
    "\n",
    "### Node error measures\n",
    "\n",
    "Given node $t$ and a numerical value $c_t$ that is used as a constant prediction at $t$, an error measure $e(t,c_t)$ can be defined by:\n",
    "\n",
    "**mean square error:** $\\dfrac{1}{|\\mathcal{I}(t)|}\\sum\\limits_{i\\in \\mathcal{I}(t)} \\left( y_i - c_t\\right)^2$ \n",
    "\n",
    "**mean absolute error:** $\\dfrac{1}{|\\mathcal{I}(t)|}\\sum\\limits_{i\\in \\mathcal{I}(t)} | y_i - c_t|$ \n",
    "\n",
    "where $\\mathcal{I}(t)$ is the set of samples fallen in node $t$. The mean or the median of the response variable values over $\\mathcal{I}(t)$ are commonly used as $c_t$ value. The same choice for $c_t$ is usually made for all the nodes.\n",
    "\n",
    ">**Task 1: (2 Points)** Complete the below functions to define the two node error measures above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26196f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_square_error(y_t,c_t):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        y_t (|I(t)|-dimensional vector): response variable values at a given node t\n",
    "        c_t (string): prediction label at node t, {'mean','median'} \n",
    "    Outputs:\n",
    "        e_tc (float): mean square error at node t using label c\n",
    "    \"\"\"\n",
    "    # insert your code here \n",
    "\n",
    "    return e_tc\n",
    "\n",
    "def mean_absolute_error(y_t,c_t):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        y_t (|I(t)|-dimensional vector): response variable values at a given node t\n",
    "        c_t (string): prediction at node t, {'mean','median'} \n",
    "    Outputs:\n",
    "        e_tc (float): mean absolute error at node t using label c\n",
    "    \"\"\"\n",
    "    # insert your code here \n",
    "\n",
    "    return e_tc\n",
    "\n",
    "print('The mean square error using the mean and the median for the whole dataset are, respectively,')\n",
    "print(np.around(mean_square_error(y,'mean'),3))\n",
    "print('and')\n",
    "print(np.around(mean_square_error(y,'median'),3))\n",
    "print('The mean absolute error using the mean and the median for the whole dataset are, respectively,')\n",
    "print(np.around(mean_absolute_error(y,'mean'),3))\n",
    "print('and')\n",
    "print(np.around(mean_absolute_error(y,'median'),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0149a2",
   "metadata": {},
   "source": [
    "Since the fundamental idea is to produce more and more accurate nodes, the selection of the split of a parent node will be done in terms of the information gain, which measures somehow the prediction accuracy gained when a node is split into child nodes. In what follows, this concept is formally defined.\n",
    "\n",
    "### Information gain\n",
    "\n",
    "Let $s\\in S$ be a candidate split of data points assigned to a given node $t$, and let $e(t,c_t)$ be a node error measure. The information gain of $s$ relative to node $t$ is defined as the average reduction of prediction error obtained by splitting the observations within node $t$ according to $s$\n",
    "\n",
    "$$ G(t,s) = e(t,c_t) - \\left(q_L(s) * e\\left(t_L(s),c_{t_L}(s)\\right) + q_R(s) * e\\left(t_R(s),c_{t_R}(s)\\right)\\right)$$\n",
    "\n",
    "where $e(t, c_t)$ denotes the error made on all predictions at node $t;$ $t_L(s)$ and $ t_R(s)$ are the left and right child nodes originating from the splitting of node $t$ produced by $s$, $c_{t_L}(s), c_{t_R}(s)$ the respective prediction labels, and $q_L(s), q_R(s)$ the proportion of observations (within node $t$) that become elements from new nodes $t_L(s), t_R(s)$. This definition could be generalized for non-binary splitting.\n",
    "\n",
    "Let $\\Omega_c$ be the set of current leaf nodes that can be potentially split. Then, the selected split over $\\Omega_c$ will be the one that maximizes the corresponding information gain:\n",
    "\n",
    "$$ G(t^\\ast, s^\\ast) = \\max\\limits_{t\\in\\Omega_c,\\,s\\in S}\\left\\lbrace G(t,s) \\right\\rbrace$$\n",
    "\n",
    ">**Task 2: (9 Points)** Complete the below function following the steps within it in order to produce the first split in a binary regression tree for all the observations of the Boston-Housing data set and for both node error measures. Consider just crim as the predictor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc0c539",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = np.array(X[:,0])\n",
    "target = y\n",
    "\n",
    "def first_split(feature, target, node_error_measure, c_t):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        feature (N-dimensional vector): feature values\n",
    "        target (N-dimensional vector): target values\n",
    "        node_error_measure (function): mean square error or mean absolute error functions\n",
    "        c_t (string): prediction label at node t, {'mean','median'} \n",
    "    Outputs:\n",
    "        error_parent_node (float): error measure of parent node t\n",
    "        error_left_child_node (float): error measure of left child node t_L\n",
    "        error_right_child_node (float): error measure of right child node t_R\n",
    "        best_cutpoint (float): feature value from which to split b_t\n",
    "    \"\"\"\n",
    "    # Step 1: Compute the list of candidate cutpoints\n",
    "    # insert your code here\n",
    "    \n",
    "    \n",
    "    # Step 2: Compute the information gain for each candidate cutpoint\n",
    "    # insert your code here\n",
    "    \n",
    "    \n",
    "    # Step 3: Obtain the first split according to the maximum information gain\n",
    "    # insert your code here\n",
    "        \n",
    "    \n",
    "    print('Node error measure used is:', node_error_measure.__name__)\n",
    "    print('with prediction based on:', c_t)\n",
    "    print('Error at root node =', np.around(error_parent_node,3))\n",
    "    print('The left branch is','X <=', np.around(best_cutpoint,3), 'with error =', np.around(error_left_child_node,3))\n",
    "    print('The right branch is','X >', np.around(best_cutpoint,3), 'with error =', np.around(error_right_child_node,3))\n",
    "    print('\\n')\n",
    "    \n",
    "    return error_parent_node, error_left_child_node, error_right_child_node, best_cutpoint\n",
    "\n",
    "first_split(feature, target, mean_square_error, 'mean');\n",
    "first_split(feature, target, mean_square_error, 'median');\n",
    "first_split(feature, target, mean_absolute_error, 'mean');\n",
    "first_split(feature, target, mean_absolute_error, 'median');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b6e988-f658-4e23-bf19-5bf9a041918d",
   "metadata": {},
   "source": [
    ">**Question 1: (1 Points)** Explain in one sentence the link between the outputs obtained from Tasks 1 and 2.\n",
    ">\n",
    ">*Answer* (Include your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46b99d0",
   "metadata": {},
   "source": [
    ">**Task 3: (3 Points)** Use <a href=\"https://scikit-learn.org/dev/modules/generated/sklearn.tree.DecisionTreeRegressor.html\">DecisionTreeRegressor</a>  from the scikit-learn package to learn the first split for both mean square error and mean absolute error using the mean and the median as predictions, respectively.  Consider just crim as the predictor again. Are the results consistent with the output obtained in Task 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6ba9f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# insert your code here\n",
    "\n",
    "plt.figure()\n",
    "plot_tree(clf_mse_mean);\n",
    "\n",
    "# insert your code here\n",
    "\n",
    "plt.figure()\n",
    "plot_tree(clf_mae_median);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cc40aa-0aa7-4118-a4e6-2cac4f15e08c",
   "metadata": {},
   "source": [
    "## B. Bagging Trees\n",
    "\n",
    "Single decision trees are known to suffer from high variance. This means that if two decision trees are grown over two disjoint subsamples from the training data, they may lead to quite different results. A general procedure for reducing the variance of any learning method is bootstrap aggregating or bagging. *A bootstrap is a set of observations extracted randomly from the original sample, with replacement and of the same size as the original sample.* Bagging is a methodology that generates multiple bootstrap samples and the regressor is trained with each one and then the outcomes are aggregated. \n",
    "\n",
    ">**Question 2: (3 Points)** Compute the probability that a given observation in a data set of size N is part of a bootstrap sample, and study the limit of this probability as N tends to infinity.\n",
    ">\n",
    ">*Answer* (Include your answer here)\n",
    "\n",
    "\n",
    "A special case of bagging is the well-known Random Forest model. In the Random Forest model, B decision trees are grown over B bootstrapped training samples and the prediction is the averaeg tadditionally, at each time the split ata  node in a given decision tree is considered, a sample of $m$ predictor variables is chosen randomly among the $d$ initial ones. This randomization helps to decorrelate the trees that are being generated.\n",
    "\n",
    "An advantage of bagging is that all the trees are grown independently, and hence they can be fit in parallel.\n",
    "\n",
    ">**Task 4: (5 Points)** Implement the Random Forest regressor using B = 10 and $m=\\lceil \\log_2{d} \\rceil$, where np.random.choice might be helpful. You can use <a href=\"https://scikit-learn.org/dev/modules/generated/sklearn.tree.DecisionTreeRegressor.html\">DecisionTreeRegressor</a> from scikit-learn package together with the fit and predict functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c425291",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "def train_and_predict_RF(B, m, X_train, y_train, X_test):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        B (int): number of trees\n",
    "        m (int): number of features to be randomly chosen at each split in the tree\n",
    "        X_train (Nxp matrix): training data\n",
    "        y_train (N vector): response values\n",
    "        X_test (N_test x p matrix): test data to be predicted\n",
    "    Outputs:\n",
    "        y_train_pred (N_train x B matrix): prediction for each individual in the training data as a function of B\n",
    "        y_test_pred (N_test x B matrix): prediction for each individual in the test data as a function of B\n",
    "    \"\"\"\n",
    "    # insert your code here\n",
    "\n",
    "    return y_train_pred, y_test_pred\n",
    "\n",
    "np.random.seed(1)\n",
    "B = 10\n",
    "y_train_pred_RF, y_test_pred_RF = train_and_predict_RF(B, int(np.round(np.log2(13))), X_train, y_train, X_test)\n",
    "mse_test_10 = np.sum((y_test-y_test_pred_RF[:,B-1])**2)/len(y_test)\n",
    "print('The RF mse over the test sample is:', mse_test_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c318828-9e9b-43fa-832e-6627a67382b7",
   "metadata": {},
   "source": [
    ">**Task 5: (2 Points)** Plot the evolution of the mean square error of both the train and test sample as a function of the number of trees $B\\in \\{1,\\ldots, 10\\}$ of the Random Forest obtained in Task 4. Include the performance of one single full decision tree obtained from <a href=\"https://scikit-learn.org/dev/modules/generated/sklearn.tree.DecisionTreeRegressor.html\">DecisionTreeRegressor</a> routine, using the mean square error as node error measure, the mean as node prediction and the minimum number of samples required to split a node equal to $10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7f1bc7-2b19-4173-8ccb-230c43e14b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test mse for RF\n",
    "mse_train_RF = np.zeros(B)\n",
    "mse_test_RF = np.zeros(B)\n",
    "# insert your code here\n",
    "\n",
    "\n",
    "# train and test mse for tree\n",
    "# insert your code here\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(1,B+1),mse_train_RF, 'b-', label=\"Training RF\")\n",
    "plt.plot(np.arange(1,B+1),mse_test_RF, 'r-', label=\"Test RF\")\n",
    "plt.plot(np.arange(1,B+1),np.repeat(mse_train_tree,B), 'b--', label=\"Training tree\")\n",
    "plt.plot(np.arange(1,B+1),np.repeat(mse_test_tree,B), 'r--', label=\"Test tree\")\n",
    "plt.xticks(np.arange(1,B+1))\n",
    "plt.xlabel('Number of trees')\n",
    "plt.ylabel('Mean square error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0e1a45-60d8-47f7-9978-88c638b0a495",
   "metadata": {},
   "source": [
    ">**Question 3: (2 Points)** Compare the performance of the random forest regressor and the single full decision tree in terms of goodness-of-fit and generalization power in the following cases, and explain briefly why this might be happening:\n",
    ">- when $B=1$.\n",
    ">- when $B$ becomes larger and larger.\n",
    ">  \n",
    ">*Answer* (Include your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d2e528",
   "metadata": {},
   "source": [
    "In the following, we will study a proximity measure for a given pair of observations designed for Random Forest. Let $i$ and $j$ be two observations, the similarity measure between both, $\\delta_{ij}$, is the proportion of trees in the RF in which $i$ and $j$ are placed in the same leaf node. This proximity measure is usually calculated during the construction of RF by making use of the OOB observations. Then, the proportion is not calculated using all B trees but only using the number of trees where observations $i$ and $j$ are in the OOB observations.\n",
    "\n",
    ">**Task 6: (6 Points)** Complete the function below in order to compute the Random Forest similarity measure for each pair of observations at the same time of model fitting.  You can use <a href=\"https://scikit-learn.org/dev/modules/generated/sklearn.tree.DecisionTreeRegressor.html\">DecisionTreeRegressor</a> from scikit-learn package, where you specify the number of features to be randomly chosen at each split in the tree and the minimum number of samples in a node to perform a split, together with the fit and predict functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fa8a9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def similarity_measure_RF(B, m, msl, X, y):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        B (int): number of trees\n",
    "        m (int): number of features to be randomly chosen at each split in the tree\n",
    "        msl (int): mininum number of samples in a node to perform a split\n",
    "        X (Nxd matrix): feature values\n",
    "        y (N vector): response values\n",
    "    Outputs:\n",
    "        delta (NxN matrix): similarity matrix\n",
    "        y_pred (N vector): prediction for each individual in the data\n",
    "    \"\"\"\n",
    "    #insert your code here\n",
    "\n",
    "    return delta, y_pred\n",
    "\n",
    "np.random.seed(1)\n",
    "delta_50, y_pred_50 = similarity_measure_RF(100, int(np.round(np.log2(8))), 50, X, y)\n",
    "delta_10, y_pred_10 = similarity_measure_RF(100, int(np.round(np.log2(8))), 10, X, y)\n",
    "\n",
    "fig, plts = plt.subplots(1,2, figsize=(12,5)) \n",
    "fig1=plts[0].matshow(delta_50, cmap='Blues')\n",
    "plts[0].set_title('Similarity matrix (msl=50)')\n",
    "fig2=plts[1].matshow(delta_10, cmap='Blues')\n",
    "plts[1].set_title('Similarity matrix (msl=10)')\n",
    "plt.tight_layout() \n",
    "plt.show()\n",
    "\n",
    "print('The similarity between observations 9 and 10 with msl=50 is:', delta_50[8,9]) \n",
    "print('The similarity between observations 9 and 10 with msl=10 is:', delta_10[8,9]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9dd1de-52f1-441f-abff-c2899e5ae76b",
   "metadata": {},
   "source": [
    ">**Question 4: (1 Points)** How can you explain the different intensities obtained with both heatmaps in Task 6?\n",
    ">\n",
    ">*Answer* (Include your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70fc009-ff3f-463a-a315-b6371c306709",
   "metadata": {},
   "source": [
    "Next, we will use the data proximity matrix provided by Random Forests in order to do some data visualization via MultiDimensional Scaling (MDS). MDS uses as input any dissimilarity matrix. The Random Forests dissimilarity matrix is defined as $\\bar{\\delta}_{ij} = \\sqrt{1-\\delta_{ij}},\\,\\, i,j=1,\\ldots,N$.\n",
    "\n",
    ">**Task 7: (3 Points)** Apply multidimensional scaling to the dissimilarity matrix using <a href=\"https://scikit-learn.org/dev/modules/generated/sklearn.manifold.MDS.html\">MDS</a> from scikit-learn package. Then, plot the first two coordinates as a function of both the value of the response variable and the predicted values obtained in Task 6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcfba61-c571-4204-abc8-da39fd5b0635",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "# insert your code here\n",
    "\n",
    "\n",
    "fig, plts = plt.subplots(2,2, figsize=(12,10)) \n",
    "\n",
    "fig1=plts[0,0].scatter(coordinates_MDS_50[:,0], coordinates_MDS_50[:,1], c=y, cmap='Blues')\n",
    "plts[0,0].set_xlabel('Coordinate 1')\n",
    "plts[0,0].set_ylabel('Coordinate 2')\n",
    "plts[0,0].set_title('MDS w.r.t. real target values (msl=50)')\n",
    "fig.colorbar(fig1, ax=plts[0,0], label='medv')\n",
    "\n",
    "fig2=plts[0,1].scatter(coordinates_MDS_50[:,0], coordinates_MDS_50[:,1], c=y_pred_50, cmap='Blues')\n",
    "plts[0,1].set_xlabel('Coordinate 1')\n",
    "plts[0,1].set_ylabel('Coordinate 2')\n",
    "plts[0,1].set_title('MDS w.r.t. predicted target values (msl=50)')\n",
    "fig.colorbar(fig2, ax=plts[0,1], label='medv_pred')\n",
    "\n",
    "fig3=plts[1,0].scatter(coordinates_MDS_10[:,0], coordinates_MDS_10[:,1], c=y, cmap='Blues')\n",
    "plts[1,0].set_xlabel('Coordinate 1')\n",
    "plts[1,0].set_ylabel('Coordinate 2')\n",
    "plts[1,0].set_title('MDS w.r.t. real target values (msl=10)')\n",
    "fig.colorbar(fig3, ax=plts[1,0], label='medv')\n",
    "\n",
    "fig4=plts[1,1].scatter(coordinates_MDS_10[:,0], coordinates_MDS_10[:,1], c=y_pred_10, cmap='Blues')\n",
    "plts[1,1].set_xlabel('Coordinate 1')\n",
    "plts[1,1].set_ylabel('Coordinate 2')\n",
    "plts[1,1].set_title('MDS w.r.t. predicted target values (msl=10)')\n",
    "fig.colorbar(fig4, ax=plts[1,1], label='medv_pred')\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c10c7a-aba5-4838-8035-fbee7648ef23",
   "metadata": {},
   "source": [
    ">**Question 5: (3 Points)** Compare the outputs observed in Task 6 for msl=50 and msl=10, in terms of:\n",
    ">- the  structure according to the actual target values;\n",
    ">- the goodness-of-fit according to predicted target values.\n",
    ">\n",
    ">*Answer* (Include your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9275055-90eb-4b33-af8b-2bf43cf5a46d",
   "metadata": {},
   "source": [
    "In the following, we will study a method to compute feature contributions for model predictions based on the <a href=\"https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf\">Shapley value</a>, a measure taken from cooperative game theory. This method requires retraining the model on all feature subsets $S\\subseteq F$, where $F$ is the set of all features. It assigns an importance value to each feature that represents the effect on the model prediction including that feature. To compute this effect, a model $f_{S\\cup j}$ is trained with that feature present, and another model $f_S$ is trained with the feature withheld. Then, predictions from the two models are compared on the current input $f_{S\\cup i} (x^i_{S\\cup j}) - f_S(x^i_S)$, where $x^i_S$ represents the values of the input features in the set $S$ for observation $x^i$. Since the effect of withholding a feature depends on other features in the model, the preceding differences are computed for all possible subsets $ S\\subseteq F \\backslash j$. The Shapley values $\\Phi_j(x^i)$ are then computed and used as feature attributions. They are a weighted average of all possible differences:\n",
    "\n",
    "$$\\Phi_j(x^i) = \\sum\\limits_{S\\subseteq F\\backslash j} \\dfrac{|S|!\\left(|F|-|S|-1\\right)!}{|F|!}\\left[ f_{S\\cup j}\\left(x^i_{S\\cup j}\\right)-f_S\\left(x^i_S\\right)\\right].$$\n",
    "\n",
    "Then, one could define the importance of a given feature $j$ as the mean of its Shapley values over the whole dataset, that is,\n",
    "\n",
    "$$I_j = \\frac{1}{n}\\sum\\limits_{i=1}^n |\\Phi_j(x^i)|.$$\n",
    "\n",
    ">**Question 6: (1 Points)** An exact computation of the Shapley values is computationally expensive. How many models one would have to generate to obtain the Shapley values of all the features in our example, i.e., the Boston-Housing dataset?\n",
    ">\n",
    ">*Answer* (Include your answer here)\n",
    "\n",
    "Due to the computational effort required to obtain the Shapley values, we will work with an approximation. Instead of computing the models for all the possible subsets of features $S\\subseteq F$, we will sample a small collection of them, namely, $k$, thus the sum in $\\Phi_j(x^i)$ will consist of just $k$ terms. In the following, we will refer to $k$ as the approximation size of the Shapley values. \n",
    "\n",
    ">**Task 8: (7 Points)** Complete the function below in order to compute the Shapley value of all the predictors and all the individuals in the training set, and the corresponding feature importance. Make use of the <a href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestRegressor.html\">RandomForestRegressor</a> routine from scikit-learn package together with the fit and predict functions. Use the default parameters and random_state=0. The function \"sample\" from package \"random\" might be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffbc6af-acaa-49b9-8222-de52b736e5eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def shapley_values_RF(X, y, k):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        X (Nxd matrix): feature data\n",
    "        y (N vector): response values\n",
    "        k (int): approximation size\n",
    "    Outputs:\n",
    "        Phi (N x p vector): Shapley value per observation per predictor \n",
    "        I (p vector): Shapley variable importance\n",
    "    \"\"\"\n",
    "    clf = RandomForestRegressor(random_state=0)\n",
    "    #insert your code here\n",
    "    \n",
    "    return Phi, I\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "Phi, I = shapley_values_RF(X_train, y_train, 10)\n",
    "\n",
    "fig, plts = plt.subplots(1,2, figsize=(12,5)) \n",
    "\n",
    "fig1=plts[0]\n",
    "for j in range(13):\n",
    "    plts[0].scatter(Phi[:,j],(j+1)*np.ones(len(y_train)),s=5)\n",
    "plts[0].set_xlabel('Shapley values approximations')\n",
    "plts[0].set_yticks(np.arange(1,14), features)\n",
    "plts[0].set_title('Feature contribution to individual predictions')\n",
    "\n",
    "fig2=plts[1].bar(np.arange(1,14), I/np.sum(I))\n",
    "plts[1].set_xticks(np.arange(1,14), features, rotation=90)\n",
    "plts[1].set_title('Normalized Feature Importance')\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()\n",
    "\n",
    "print('The most important feature is', features[np.argmax(I)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ac2ff0-580e-4ff3-9890-0a4f2d741f28",
   "metadata": {},
   "source": [
    "A popular and less computationally expensive method is <a href=\"https://shap.readthedocs.io/en/latest/\">SHAP</a> that is an alternative method to estimate Shapley values.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80b85e5",
   "metadata": {},
   "source": [
    "## C. Boosting Trees\n",
    "\n",
    "The motivation for boosting is to combine the outputs of many weak regressors to produce a robust model. A weak regressor is one whose error rate is only slightly better than guessing using the average. The purpose of boosting is to sequentially apply the weak regression algorithm to repeatedly modified versions of the data, thereby producing a sequence of weak regressors. The predictions from all of them are then aggregated to produce the final prediction. Contrary to bagging, boosting cannot be parallelized because each new version of the data that is dependent on the previous output. \n",
    "\n",
    "One of the most popoular boosting algorithm is <a href=\"https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=d186abec952c4348870a73640bf849af9727f5a4\">AdaBoost</a>. In AdaBoost, the data modifications at each boosting step consist of assigning different weights to each of the training observations, forcing each successive regressor to concentrate on the ones that were poorly fitted by previous models in the sequence.\n",
    "\n",
    "Gradient Boosting is a generalization of AdaBoost to a statistical framework that treats the training process as an additive model and allows arbitrary differentiable loss functions to be optimized using gradient descent.\n",
    "\n",
    "<a href=\"https://arxiv.org/pdf/1603.02754.pdf\">Extreme Gradient Boosting</a> (XGBoost), cited 44998 times since publication in 2016, and <a href=\"https://papers.nips.cc/paper_files/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf\">Light Gradient Boosting Machines</a> (LightGBM), cited 13871 times since publication in 2017, are two popular efficient implementations of Gradient Boosting. \n",
    "\n",
    ">**Task 9: (2 Points)** Implement both XGBoost and LightGBM with 1000 trees at maximum depth $1$, and compare their test mean square error over the Boston-Housing dataset. You can use XGBRegressor from xgboost package, and LGBMRegressor from lightgbm package with a learning rate of 0.15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cfaca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "# insert your code here\n",
    "\n",
    "print('The XGBoost mse over the test sample is:', mse_test_xgbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437e5358",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "# insert your code here\n",
    "\n",
    "print('The LightGBM mse over the test sample is:', mse_test_lgbmr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
